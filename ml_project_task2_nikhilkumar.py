# -*- coding: utf-8 -*-
"""ML_Project_Task2_NikhilKumar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QF-WOXdS8mZKC9prhdCsxtF807e-aVNt
"""

#STEP BY STEP DONE CODE with Preprocessing Bigram of Characters
#lemmatization may not have a significant impact on the output

import re
import random
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score

def load_dataset(file_path):
    # Load dataset from file
    with open(file_path, 'r') as file:
        dataset = file.readlines()
    return dataset

def preprocess_message(message, lemmatizer):
    # Tokenization
    tokens = list(message.lower())

    # Lemmatization
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]

# In this context of processing individual characters, lemmatization may not have a significant impact on the output
# because lemmatization is typically applied to words to reduce them to their base or dictionary form.
# Even thoug it does not much affect the output it helps maintain a consistent preprocessing pipeline and
# could potentially improve performance

    # Add start and end of message tokens
    lemmatized_tokens = ['<s>'] + lemmatized_tokens + ['</s>']
    return lemmatized_tokens

def preprocess_dataset(dataset, lemmatizer):
    preprocessed_data = []
    for line in dataset:
        label, message = line.strip().split('\t')
        preprocessed_message = preprocess_message(message, lemmatizer)
        preprocessed_data.append((label, preprocessed_message))
    return preprocessed_data

def split_dataset(dataset, split_ratio=0.8):
    # Shuffle dataset
    # random.shuffle(dataset)

    # Split dataset into training and testing
    split_index = int(len(dataset) * split_ratio)
    training_data = dataset[:split_index]
    testing_data = dataset[split_index:]

    return training_data, testing_data

def separate_messages_by_label(dataset):
    spam_messages = []
    ham_messages = []
    for label, message in dataset:
        if label == 'spam':
            spam_messages.append(message)
        else:
            ham_messages.append(message)
    return spam_messages, ham_messages

def train_frequency_model(messages):
    bigram_frequencies = defaultdict(int)
    total_bigrams = 0

    # Count bigram frequencies, Also tried Trigram, commented once are for trigram
    for message in messages:

        for i in range(len(message) - 1):
            bigram = (message[i], message[i+1])
        #for i in range(len(message) - 2): #tried Trigram as well but function name kept same Bigram for easiness
            #bigram = (message[i], message[i+1], message[i + 2]) #tried Trigram as well but function name kept same Bigram for easiness

            bigram_frequencies[bigram] += 1
            total_bigrams += 1

    # Convert frequencies to probabilities
    bigram_probabilities = defaultdict(float)
    for bigram, frequency in bigram_frequencies.items():
        bigram_probabilities[bigram] = frequency / total_bigrams

    return bigram_probabilities

def calculate_log_probability(message, model):
    log_prob = 0.0

    for i in range(len(message) - 1):
        bigram = (message[i], message[i+1])

    #for i in range(len(message) - 2): #tried Trigram as well but function name kept same Bigram for easiness
        #bigram = (message[i], message[i+1], message[i + 2]) #tried Trigram as well but function name kept same Bigram for easiness

        if bigram in model:
            log_prob += np.log(model[bigram])
        else:
    # Laplace smoothing used  tp avoin 0 probablity , occurs when n grams in test dataset were not seen in training data
            log_prob += np.log(1e-10)  # Add a very small value to avoid log(0)
    return log_prob

def predict_label(message, spam_model, ham_model):
    spam_log_prob = calculate_log_probability(message, spam_model)
    ham_log_prob = calculate_log_probability(message, ham_model)
    if spam_log_prob > ham_log_prob:
        return 'spam'
    else:
        return 'ham'

def evaluate_model(testing_data, spam_model, ham_model):
    true_labels = [label for label, _ in testing_data]
    predicted_labels = [predict_label(message, spam_model, ham_model) for _, message in testing_data]
    accuracy = accuracy_score(true_labels, predicted_labels)
    confusion_mat = confusion_matrix(true_labels, predicted_labels, labels=['ham', 'spam'])
    return accuracy, confusion_mat

#------------- All Evalution Parameters------------

def precision_from_confusion_matrix(confusion_mat):
    num_class = confusion_mat.shape[0]
    precision = np.zeros(num_class)
    for i in range(num_class):
        true_positives = confusion_mat[i, i]
        false_positives = np.sum(confusion_mat[:, i]) - true_positives
        precision[i] = true_positives / (true_positives + false_positives) \
            if (true_positives + false_positives) != 0 else 0

    return precision


def recall_from_confusion_matrix(confusion_mat):
    num_classes = confusion_mat.shape[0]
    recall = np.zeros(num_classes)
    for i in range(num_classes):
        true_positives = confusion_mat[i, i]
        false_negatives = np.sum(confusion_mat[i, :]) - true_positives
        recall[i] = true_positives / (true_positives + false_negatives) \
            if (true_positives + false_negatives) != 0 else 0
    return recall


def f1_score_from_confusion_matrix(confusion_mat):
    num_classes = confusion_mat.shape[0]
    f1_scores = np.zeros(num_classes)

    for i in range(num_classes):
        true_positives = confusion_mat[i, i]
        false_positives = np.sum(confusion_mat[:, i]) - true_positives
        false_negatives = np.sum(confusion_mat[i, :]) - true_positives
        precision = true_positives / (true_positives + false_positives) \
            if (true_positives + false_positives) != 0 else 0
        recall = true_positives / (true_positives + false_negatives) \
            if (true_positives + false_negatives) != 0 else 0
        f1_scores[i] = 2 * (precision * recall) / (precision + recall) \
            if (precision + recall) != 0 else 0

    return f1_scores


def macro_average_precision_score(Precision):
    num_class = len(Precision)
    macro_average_P = sum(Precision) / num_class
    return macro_average_P


def recall_macro_average(Recall):
    num_class = len(Recall)
    macro_average_R = sum(Recall) / num_class
    return macro_average_R



def F1_macro_average(Precision, Recall):
    total_precision = sum(Precision)
    total_recall = sum(Recall)
    macro_precision = total_precision / len(Precision)
    macro_recall = total_recall / len(Recall)
    macro_average_F1 = (2 * macro_precision * macro_recall) / (macro_precision + macro_recall)

    return macro_average_F1

# Load dataset
file_path = '/content/drive/MyDrive/Colab Notebooks/SMSSpamCollection'
dataset = load_dataset(file_path)

# Initialize WordNet lemmatizer
lemmatizer = WordNetLemmatizer()

# Preprocess dataset
preprocessed_dataset = preprocess_dataset(dataset, lemmatizer)

# Split dataset into training and testing
training_data, testing_data = split_dataset(preprocessed_dataset)
print("Training dataset size:", len(training_data))
print("Testing dataset size:", len(testing_data))

# Separate spam and ham messages from the training dataset
spam_messages, ham_messages = separate_messages_by_label(training_data)
print("\nSPAM Messages in training set are:",len(spam_messages))
print("HAM Message in training set are:",len(ham_messages))

# Train frequency model for spam messages
spam_model = train_frequency_model(spam_messages)
#print(spam_model)

# Train frequency model for ham messages
ham_model = train_frequency_model(ham_messages)
#print(ham_model)

# Evaluate model on testing data
accuracy, confusion_mat = evaluate_model(testing_data, spam_model, ham_model)

print("\nAccuracy:", accuracy)
print("\nConfusion Matrix:\n", confusion_mat)

# Calculate Precision, Recall, F1 Score, and Macro Average Precision
Precision = precision_from_confusion_matrix(confusion_mat)
print("\nPrecision for each class:\n", Precision)

Recall = recall_from_confusion_matrix(confusion_mat)
print("Recall for each class:\n", Recall)

F1_Score = f1_score_from_confusion_matrix(confusion_mat)
print("F1 Score for each class:\n", F1_Score)

Macro_Avg_Precision = macro_average_precision_score(precision_from_confusion_matrix(confusion_mat))
print("\nMacro Average Precision:", Macro_Avg_Precision)

Macro_Avg_Recall = recall_macro_average(Recall)
print("Macro Average Recall: ", Macro_Avg_Recall)

Macro_Avg_F1_Score = F1_macro_average(Precision, Recall)
print("Macro Average F1 Score:", Macro_Avg_F1_Score)